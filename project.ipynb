{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"project.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPSgqO4KKqQpAKYwtyy4eCQ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"p-9T-KCFxDKe"},"source":["# part 1\r\n","\r\n","!pip install hazm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rc7xX1f7g-uI"},"source":["# part 2\r\n","\r\n","import numpy as np\r\n","import pandas as pd\r\n","\r\n","import tensorflow as tf\r\n","from tensorflow.keras.preprocessing.text import Tokenizer\r\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n","from tensorflow.keras import Sequential\r\n","from tensorflow.keras.layers import Flatten, Dense, Embedding, Conv1D, GlobalMaxPooling1D, Dropout\r\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\r\n","\r\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\r\n","from mlxtend.plotting import plot_confusion_matrix\r\n","from sklearn.preprocessing import OneHotEncoder\r\n","from sklearn.model_selection import train_test_split\r\n","\r\n","import re\r\n","import nltk\r\n","nltk.download('punkt')\r\n","\r\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RxEdyV_TzeGe"},"source":["# part 3 \r\n","\r\n","from google.colab import drive \r\n","drive.mount('/content/drive', force_remount=True)\r\n","df = pd.read_csv('/content/drive/My Drive/train.csv')\r\n","col = ['Category', 'Text']\r\n","df = df[col]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xTTNEOTDhomj","executionInfo":{"status":"ok","timestamp":1611035518111,"user_tz":-210,"elapsed":411922,"user":{"displayName":"Kimia Sedighi","photoUrl":"","userId":"18261227966837517206"}}},"source":["# part 4\r\n","\r\n","from nltk.tokenize import word_tokenize\r\n","\r\n","# remove blank rows\r\n","df['Text'].dropna(inplace=True)\r\n","\r\n","# Tokenization : in this each entry in the df will be broken into set of words\r\n","df['Text']= [word_tokenize(entry) for entry in df['Text']]\r\n","\r\n","df['Text'] = df[\"Text\"].map(' '.join)\r\n","\r\n","df['Text'] = [(t.replace('\\n',' ')\r\n","            .replace('\\r',' ')\r\n","            .replace('\\t',' ')\r\n","            .replace('  ',' ')\r\n","            .strip()) for t in df['Text']]"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"DohFND9a-s-K","executionInfo":{"status":"ok","timestamp":1611035651463,"user_tz":-210,"elapsed":545271,"user":{"displayName":"Kimia Sedighi","photoUrl":"","userId":"18261227966837517206"}}},"source":["# part 5 \r\n","\r\n","from __future__ import unicode_literals\r\n","from hazm import *\r\n","\r\n","normalizer = Normalizer()\r\n","df['Text'] = df['Text'].map(lambda x: normalizer.normalize(x))\r\n","\r\n","stemmer = Stemmer()\r\n","df['Text'] = df['Text'].map(lambda x: stemmer.stem(x))\r\n","\r\n","lemmatizer = Lemmatizer()\r\n","df['Text'] = df['Text'].map(lambda x: lemmatizer.lemmatize(x))"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"-fBtzvpFck8H","executionInfo":{"status":"ok","timestamp":1611035651469,"user_tz":-210,"elapsed":545274,"user":{"displayName":"Kimia Sedighi","photoUrl":"","userId":"18261227966837517206"}}},"source":["# part 6\r\n","\r\n","texts = df[\"Text\"].values\r\n","labels = df[[\"Category\"]].values\r\n","X_train, y_train, X_test, y_test = train_test_split(texts, labels, test_size = 0.2, random_state = 42)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"YC_isJlF38T5","executionInfo":{"status":"ok","timestamp":1611035651471,"user_tz":-210,"elapsed":545274,"user":{"displayName":"Kimia Sedighi","photoUrl":"","userId":"18261227966837517206"}}},"source":["# part 7\r\n","\r\n","# the maximum number of words to be used(most frequent)\r\n","vocab_size = 50000\r\n","\r\n","# dimension of the dense embedding\r\n","embedding_dim = 130\r\n","\r\n","# truncate and padding options\r\n","trunc_type = 'post'\r\n","padding_type = 'post'\r\n","oov_tok = '<OOV>'"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"nu293j3x6E-x","executionInfo":{"status":"ok","timestamp":1611035762933,"user_tz":-210,"elapsed":656732,"user":{"displayName":"Kimia Sedighi","photoUrl":"","userId":"18261227966837517206"}}},"source":["# part 8\r\n","\r\n","tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\r\n","tokenizer.fit_on_texts(X_train)\r\n","word_index = tokenizer.word_index\r\n","\r\n","max_length = max([len(s.split()) for s in X_train])\r\n","\r\n","train_seq = tokenizer.texts_to_sequences(X_train)\r\n","train_padded = pad_sequences(train_seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)\r\n","\r\n","validation_seq = tokenizer.texts_to_sequences(y_train)\r\n","validation_padded = pad_sequences(validation_seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"NSJ-AN-y-vOW","executionInfo":{"status":"ok","timestamp":1611035762948,"user_tz":-210,"elapsed":656744,"user":{"displayName":"Kimia Sedighi","photoUrl":"","userId":"18261227966837517206"}}},"source":["# part 9\r\n","\r\n","encode = OneHotEncoder()\r\n","\r\n","training_labels = encode.fit_transform(X_test)\r\n","validation_labels = encode.transform(y_test)\r\n","\r\n","training_labels = training_labels.toarray()\r\n","validation_labels = validation_labels.toarray()"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"iju0aAbQ-76k"},"source":["# part 10\r\n","\r\n","model = Sequential()\r\n","\r\n","model.add(Embedding(vocab_size, embedding_dim, input_length=train_padded.shape[1]))\r\n","model.add(Conv1D(128, 8, activation='relu', padding='valid'))\r\n","model.add(GlobalMaxPooling1D())\r\n","model.add(Dropout(0.5))\r\n","\r\n","model.add(Flatten())\r\n","model.add(Dropout(0.5))\r\n","\r\n","model.add(Dense(34, activation='softmax'))\r\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n","\r\n","epochs = 5\r\n","batch_size = 45\r\n","\r\n","history = model.fit(train_padded, training_labels, shuffle=True ,\r\n","                    epochs=epochs, batch_size=batch_size, \r\n","                    validation_split=0.2,\r\n","                    callbacks=[ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001), \r\n","                               EarlyStopping(monitor='val_loss', mode='min', patience=2, verbose=1),\r\n","                               EarlyStopping(monitor='val_accuracy', mode='max', patience=5, verbose=1)])\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nSGVa87cARHP","executionInfo":{"status":"aborted","timestamp":1611041397535,"user_tz":-210,"elapsed":6291326,"user":{"displayName":"Kimia Sedighi","photoUrl":"","userId":"18261227966837517206"}}},"source":["# part 11\r\n","\r\n","plt.title('Accuracy')\r\n","plt.plot(history.history['accuracy'], label='train')\r\n","plt.plot(history.history['val_accuracy'], label='test')\r\n","plt.legend()\r\n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MFefODlQCkf8","executionInfo":{"status":"aborted","timestamp":1611041397541,"user_tz":-210,"elapsed":6291329,"user":{"displayName":"Kimia Sedighi","photoUrl":"","userId":"18261227966837517206"}}},"source":["# part 12\r\n","\r\n","test = pd.read_csv('/content/drive/My Drive/test.csv')\r\n","col2 = ['Id', 'Text']\r\n","test = test[col2]\r\n","\r\n","\r\n","test['Text'].dropna(inplace=True)\r\n","\r\n","test['Text'] = [word_tokenize(entry) for entry in test['Text']]\r\n","\r\n","test['Text'] = test['Text'].map(' '.join)\r\n","\r\n","test['Text'] = [(t.replace('\\n',' ')\r\n","            .replace('\\r',' ')\r\n","            .replace('\\t',' ')\r\n","            .replace('  ',' ')\r\n","            .strip()) for t in test['Text']]\r\n","\r\n","\r\n","test['Text'] = test['Text'].map(lambda x: normalizer.normalize(x))\r\n","test['Text'] = test['Text'].map(lambda x: stemmer.stem(x))\r\n","test['Text'] = test['Text'].map(lambda x: lemmatizer.lemmatize(x))\r\n","\r\n","test_x_seq = tokenizer.texts_to_sequences(test['Text'])\r\n","test_x_padded = pad_sequences(test_x_seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)\r\n","\r\n","pred = model.predict(test_x_padded)\r\n","predicted_label = encode.inverse_transform(pred)\r\n","\r\n","out = pd.DataFrame(data=predicted_label, columns=['Category'])\r\n","out.to_csv('/content/drive/My Drive/out.csv')"],"execution_count":null,"outputs":[]}]}